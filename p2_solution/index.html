<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Solution - PA-XPE-IGTI2024-AWS-Serverless-architecture</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
        <link href="../assets/_mkdocstrings.css" rel="stylesheet" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Solution";
        var mkdocs_page_input_path = "p2_solution.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> PA-XPE-IGTI2024-AWS-Serverless-architecture
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="..">Home</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../intro/">Introduction</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">Solution</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#aws-serverless-architecture">AWS Serverless Architecture</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#data-sources">Data Sources</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#data-sources-used">Data Sources Used:</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#ingestion">Ingestion</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#processtransform">Process/Transform</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#developed-glue-jobs">Developed Glue Jobs</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#data-storage">Data Storage</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#data-storage-strategy">Data Storage Strategy</a>
        <ul>
    <li class="toctree-l4"><a class="reference internal" href="#delta-format">Delta Format</a>
    </li>
        </ul>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#data-serving">Data Serving</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#others">Others 📎</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#orchestration">Orchestration</a>
    </li>
    <li class="toctree-l3"><a class="reference internal" href="#data-quality-observability">Data Quality &amp; Observability</a>
    </li>
        </ul>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../p3_ing/">Ingestion Framework</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../p4_meta/">Metabase / Dashboard</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../p5_price/">Price</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../p6_results/">Results</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../p7_referen/">References</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">PA-XPE-IGTI2024-AWS-Serverless-architecture</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Solution</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="solution">Solution</h1>
<h2 id="aws-serverless-architecture">AWS Serverless Architecture</h2>
<p><img alt="arc_img" src="../assets/images/arquiteture_draw.png" /></p>
<blockquote>
<p>💻 <strong>Note</strong>: If you are not familiar with what serverless resources are, the topic will be discussed in the price section and additional materials will be provided in the reference section.</p>
</blockquote>
<p>The goal here is not to delve into every detail of what was used, but rather to explain the core elements of the solution and some details about the concepts and tools employed. We will break it down into parts to make it easier to understand each stage.</p>
<h2 id="data-sources">Data Sources</h2>
<p><img alt="arc_img" src="../assets/images/datasources.png" /></p>
<p>For our project, we primarily used two sources (with multiple endpoints), as one might expect a financial solution to operate. We utilized a paid API that ensures data curation, availability (without slowdowns during peak hours), and high-frequency updates.</p>
<blockquote>
<p>💻 <strong>Note</strong>: Relying solely on public sources and web scraping does not guarantee quality and also demands tremendous effort. Cost-effective API solutions exist and are quite affordable; for instance, an enterprise-level service might cost about BRL 200 per month.</p>
</blockquote>
<p>For our volume and frequency, a simple service costing BRL 20 per month sufficed, saving me a great deal of time and the risk of source failure (in cases where only web scraping is used).</p>
<h3 id="data-sources-used">Data Sources Used:</h3>
<ul>
<li><strong>Brapi API</strong><ul>
<li>This included endpoints for stocks, FIIs, cryptocurrencies, historical data, and stock dividends.</li>
</ul>
</li>
<li><strong>Web scraping from a FIIs assets site</strong><ul>
<li>Focused solely on FIIs data.</li>
</ul>
</li>
</ul>
<p>The idea here was to demonstrate that our solution could ingest data in three formats: APIs, web scraping, and legacy databases. I will detail this in the ingestion section. This approach was experimental, and the <strong>logic developed</strong> could be <strong>replicated based</strong> on these established bases.</p>
<h2 id="ingestion">Ingestion</h2>
<p><img alt="ing_img" src="../assets/images/inges.png" /></p>
<p>With the data sources defined, the initial step was to construct three AWS Lambda functions:</p>
<ul>
<li><strong>Stock Data Lambda</strong>:<ul>
<li>Handles data for 10 stocks due to API call limit restrictions.</li>
</ul>
</li>
<li><strong>Crypto Data Lambda</strong>:<ul>
<li>Initially manages data for only 2 assets.</li>
</ul>
</li>
<li><strong>FIIs Data Lambda</strong> (Web Scraping):<ul>
<li>Scrapes approximately 30 assets per run.</li>
</ul>
</li>
</ul>
<p>Database Setup on AWS RDS</p>
<p>Following the Lambda setup, a PostgreSQL database was established on AWS RDS. <strong>This setup was intended to simulate the migration from a legacy database system</strong>. The database includes historical data and dividend information for stocks.</p>
<p>The aim was to address data ingestions from the most common types of sources during the ingestion phase, demonstrating the versatility of the serverless architecture.</p>
<h2 id="processtransform">Process/Transform</h2>
<p><img alt="glue_img" src="../assets/images/glue_pro.png" /></p>
<p>With the data ingested into AWS from the previous step, AWS Glue was chosen as the processing and transformation resource. This choice was made because Glue supports Spark and allows for the use of Python. </p>
<p>Considering that the solution needed to scale effortlessly, implementing it with Spark from the outset significantly increases the update frequency of the Lambdas and allows scaling the number of Lambdas without impacting the performance of the transformation and processing stage.</p>
<h4 id="developed-glue-jobs">Developed Glue Jobs</h4>
<p>Five jobs were developed in Glue:</p>
<ol>
<li><strong>Stocks Raw</strong> - Primarily involves removing duplicates from the source and saving these files in delta format.</li>
<li><strong>Webscrap Raw</strong> - Involves removing duplicates from the source and saving these files in delta format.</li>
<li><strong>Crypto Raw</strong> - Involves removing duplicates from the source and saving these files in delta format.</li>
<li><strong>Historical Dividend</strong> - Merges (with a join) the output of the first job with historical data that came from the DMS migration.</li>
<li><strong>Stocks Refined</strong> - Involves column selection, data typing treatment, and creation of calculated fields, simulating a trusted table being made available to the business.</li>
</ol>
<blockquote>
<p>🔎 <strong>NOTE</strong>: If you look at the codes, you will see only 3 scripts, because a way was developed for the first three to use the same script base, changing only their variables. All of this is automatically deployed via Terraform (variables / terraform.tfvars [hidden 🔒]).</p>
</blockquote>
<h2 id="data-storage">Data Storage</h2>
<p><img alt="storage_img" src="../assets/images/storage.png" /></p>
<h3 id="data-storage-strategy">Data Storage Strategy</h3>
<p>Data storage is a critical component at nearly every stage of the project, implemented using different object stores (AWS S3 buckets) according to the stage, following the best practices of Medallion architecture type. Data were segregated into three layers:</p>
<ul>
<li><strong>Landing</strong> - Stores the ingested data in JSON format.</li>
<li><strong>Bronze</strong> - Stores data without duplicates in delta format.</li>
<li><strong>Silver</strong> - Stores data with joins, calculated columns, and business-specific columns.</li>
</ul>
<blockquote>
<p>🔎 <strong>NOTE</strong>: A Gold layer was not constructed because such layers generally require a project to be at a more advanced stage. Often, new projects are fed by tables from the Silver zone - trusted or refined. Additionally, the project development time was short, and there wasn’t enough time to consolidate a modeling strategy.</p>
</blockquote>
<p>Having separate buckets for different layers offers several advantages, such as having a more historical layer and a more analytical one. As the project evolves, it becomes possible to determine the necessary range of data for generating value, allowing for different configurations in the S3 buckets, such as having a replica for backup purposes in the historical layer. </p>
<p>While not applied here, it is also possible to configure a range (based on dates) to store older data in a colder storage layer, meaning these less accessed data will essentially hibernate, reducing costs. These are some of the advantages of the implemented approach, and as the project progresses and more is understood about the business, even more features aimed at performance and cost reduction can be configured.</p>
<h4 id="delta-format">Delta Format</h4>
<p>In addition to the above optimizations, the Delta file format was utilized for the analytical layers, enhancing performance and reducing costs through ACID transactions and schema enforcement. These features maintain data integrity and consistency and support efficient querying with mechanisms like file skipping and Z-ordering.</p>
<p>While not all advanced features were leveraged (becuase of the time), significant learning were gained regarding the fundamental aspects of Delta Lake and its practical application within the AWS Glue environment (not trivial like Dbricks). This strategic decision supports transitioning towards a lakehouse architecture, which optimizes both batch and real-time data processing, and provides a scalable framework for future enhancements.</p>
<h2 id="data-serving">Data Serving</h2>
<p><img alt="serv_img" src="../assets/images/serving.png" /></p>
<p>Once all the data has been processed and transformed, the AWS Athena tool can access these tables, which were crawled by the Glue Crawler. This setup recognizes data structures and can significantly benefit from potential performance and cost optimizations. However, it's important to note that in this specific instance, optimizations using Hive or Delta for query enhancements were not utilized due to time constraints, although such enhancements are certainly possible.</p>
<p>Following this, an EC2 instance with 8 GB of RAM and 2 CPUs is launched to support at least 10 analysts and dashboards. This setup, which is quite performant, likely exceeds the current needs. On this Linux machine, PostgreSQL and Metabase are installed. PostgreSQL stores the state and volume data for Metabase.</p>
<p>With Metabase operational, a quick and intuitive configuration process is carried out, including the creation of user accounts and connections. Metabase is directly linked to Athena, ensuring that every query run accesses data through Athena and is served via Metabase’s interface. </p>
<p>This setup allows data or business analysts to access data without needing to directly interact with AWS, and facilitates the publishing of dashboards and queries to Google Sheets and Google Presentations.</p>
<p>This architecture not only secures high-performance data querying and low-cost operations on the silver data layers but also enhances accessibility and usability for analysts through a user-friendly dashboard interface.</p>
<blockquote>
<p>🏁 <strong>NOTE</strong> Here we reach the conclusion of the main points that were developed. Below, some descriptions about the remaining items will be provided, but there will be more discussions about the learnings encountered in the results session.</p>
</blockquote>
<h2 id="others">Others 📎</h2>
<h3 id="orchestration">Orchestration</h3>
<p><img alt="orc_img" src="../assets/images/orchestration.png" /></p>
<p>In this project, two other serverless resources were utilized, which together function similarly to Airflow. However, given the convenience of creating DAGs with AWS Step Functions and the limited time factor, this was the chosen approach for now.</p>
<p>In the cost section, it will be seen that this is undoubtedly a good option in this analysis, but in terms of management, it is believed that Airflow is more robust and a more complete tool. However, the choice was also about seizing an opportunity to learn, experiment, and particularly value time efficiency.</p>
<p>It's worth noting that this tool has an easy implementation and monitoring of DAGs similar to Airflow. Below is the dashboard showing the latest runs of a pipeline orchestrated by Step Functions:</p>
<p><img alt="orc2_img" src="../assets/images/runs_state.png" /></p>
<h3 id="data-quality-observability">Data Quality &amp; Observability</h3>
<p><img alt="qo_img" src="../assets/images/qo.png" /></p>
<p>AWS already hosts a variety of services that are essential for two crucial aspects of data lifecycle management: data quality and observability. However, the focus was more on utilizing CloudWatch and Datacatalog. Additionally, there was an exploration into developing a custom tool tailored for specific needs. It's understood that there were numerous constraints in previous roles which shaped this approach.</p>
<p>Firstly, using the Pydantic library, a sort of data contract was created, specifying some columns as mandatory during data ingestion to ensure data quality with key fields always populated.</p>
<p>Secondly, within all the Lambdas and some Glue scripts, a method was devised to monitor pertinent metrics, such as process time, file size, and log collection, verifying if they succeeded according to the set logic. These logs are stored in an S3 environment, accessed via boto3 within Docker, and visualized in Streamlit. While recognizing that this approach isn't the most efficient compared to existing, superior solutions, it was more about highlighting the importance of these metrics and prioritizing my time effectively.</p>
<p><img alt="qo2_img" src="../assets/images/moni.png" /></p>
<hr />
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../intro/" class="btn btn-neutral float-left" title="Introduction"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../p3_ing/" class="btn btn-neutral float-right" title="Ingestion Framework">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../intro/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../p3_ing/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
