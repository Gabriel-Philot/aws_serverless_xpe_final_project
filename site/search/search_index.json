{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Home \ud83d\udcbe Applied Project Doc - XPE 2024 (Final stage of the MBA - Cloud Data Engineering) Table of Contents Intro - Motivation / Objectives \ud83d\udca1 Solution - Main concepts and stacks. \ud83d\udcd0 Ingestion Framework - Main features that included some cool practices \ud83d\udccb Metabase / Dashboard - Why Metabase? / Dashboard \ud83d\udcca Price - Development Cost + Production Price Projection \ud83d\udcb5 Results - Results Achieved in the Project / Potential Business Impact \ud83c\udf93 References - References \ud83d\udcda Link for the repo: https://github.com/Gabriel-Philot/xpe_igti_pa","title":"Home"},{"location":"#home","text":"\ud83d\udcbe Applied Project Doc - XPE 2024 (Final stage of the MBA - Cloud Data Engineering)","title":"Home"},{"location":"#table-of-contents","text":"Intro - Motivation / Objectives \ud83d\udca1 Solution - Main concepts and stacks. \ud83d\udcd0 Ingestion Framework - Main features that included some cool practices \ud83d\udccb Metabase / Dashboard - Why Metabase? / Dashboard \ud83d\udcca Price - Development Cost + Production Price Projection \ud83d\udcb5 Results - Results Achieved in the Project / Potential Business Impact \ud83c\udf93 References - References \ud83d\udcda Link for the repo: https://github.com/Gabriel-Philot/xpe_igti_pa","title":"Table of Contents"},{"location":"intro/","text":"Introduction Motivation \ud83c\udf0e The goal of this project was the final stage of an MBA in Cloud Data Engineering, which is a free applied project. However, the opportunity was taken to apply something already being done at work, allowing for the introduction of various improvements/experiments and features believed to add value both from a technical and business perspective. Therefore, a hypothetical scenario similar to the data situation was recreated and sources fitting this created context were sought. Objectives \ud83d\udccc Based on the project requirements provided by the institution, we would follow a stage of creation and modeling of what we would do for our thesis. The solution was previously conceived with a design thinking bias to establish the objectives and solution that will be built. Sharing a bit of the timeline of how it was done, basically, we had the following: Two weeks of using various artifacts and design thinking techniques to shape the solution and arrive at a backlog. Three two-week sprints of hands-on work. One week to finalize results and conclusions, and we will have another week to present the project, so here I am in the middle to do the documentation and practice my presentation (24/jun/2024). From the design thinking process, a hypothetical scenario was created featuring a financial market company. While another financial market project might not seem very creative, it provided a valuable opportunity to delve into this area. Although the knowledge gained about the business side was limited, keeping up with daily news was a valuable step forward. The choice of the financial market was influenced by previous job experiences, which involved frequent micro ingestions of data from various types and sources for logistics purposes. This highlighted the similarity with the data needs in the financial sector, without the need to exhaustively chase down data. With this information, a solution architecture was designed that: Can scale without issues, meaning no matter how many assets there are or how many ingestions, it won't impact performance or data availability. Flexible, following the first point, able to scale up/down resources, with the ability to ingest data from any type of source (API, Webscrap, legacy databases etc.). Centralized data provision for clients and analysts (internal stakeholders). Low entry cost. In this context, a Serverless solution on AWS would fit, which was how I practically built everything for my project (at my previous job I basically did everything with AWS Lambda, S3, and very little Glue, so this choice was quite timely for me to improve).","title":"Introduction"},{"location":"intro/#introduction","text":"","title":"Introduction"},{"location":"intro/#motivation","text":"The goal of this project was the final stage of an MBA in Cloud Data Engineering, which is a free applied project. However, the opportunity was taken to apply something already being done at work, allowing for the introduction of various improvements/experiments and features believed to add value both from a technical and business perspective. Therefore, a hypothetical scenario similar to the data situation was recreated and sources fitting this created context were sought.","title":"Motivation \ud83c\udf0e"},{"location":"intro/#objectives","text":"Based on the project requirements provided by the institution, we would follow a stage of creation and modeling of what we would do for our thesis. The solution was previously conceived with a design thinking bias to establish the objectives and solution that will be built. Sharing a bit of the timeline of how it was done, basically, we had the following: Two weeks of using various artifacts and design thinking techniques to shape the solution and arrive at a backlog. Three two-week sprints of hands-on work. One week to finalize results and conclusions, and we will have another week to present the project, so here I am in the middle to do the documentation and practice my presentation (24/jun/2024). From the design thinking process, a hypothetical scenario was created featuring a financial market company. While another financial market project might not seem very creative, it provided a valuable opportunity to delve into this area. Although the knowledge gained about the business side was limited, keeping up with daily news was a valuable step forward. The choice of the financial market was influenced by previous job experiences, which involved frequent micro ingestions of data from various types and sources for logistics purposes. This highlighted the similarity with the data needs in the financial sector, without the need to exhaustively chase down data. With this information, a solution architecture was designed that: Can scale without issues, meaning no matter how many assets there are or how many ingestions, it won't impact performance or data availability. Flexible, following the first point, able to scale up/down resources, with the ability to ingest data from any type of source (API, Webscrap, legacy databases etc.). Centralized data provision for clients and analysts (internal stakeholders). Low entry cost. In this context, a Serverless solution on AWS would fit, which was how I practically built everything for my project (at my previous job I basically did everything with AWS Lambda, S3, and very little Glue, so this choice was quite timely for me to improve).","title":"Objectives \ud83d\udccc"},{"location":"p2_solution/","text":"Solution AWS Serverless Architecture \ud83d\udcbb Note : If you are not familiar with what serverless resources are, the topic will be discussed in the price section and additional materials will be provided in the reference section. The goal here is not to delve into every detail of what was used, but rather to explain the core elements of the solution and some details about the concepts and tools employed. We will break it down into parts to make it easier to understand each stage. Data Sources For our project, we primarily used two sources (with multiple endpoints), as one might expect a financial solution to operate. We utilized a paid API that ensures data curation, availability (without slowdowns during peak hours), and high-frequency updates. \ud83d\udcbb Note : Relying solely on public sources and web scraping does not guarantee quality and also demands tremendous effort. Cost-effective API solutions exist and are quite affordable; for instance, an enterprise-level service might cost about BRL 200 per month. For our volume and frequency, a simple service costing BRL 20 per month sufficed, saving me a great deal of time and the risk of source failure (in cases where only web scraping is used). Data Sources Used: Brapi API This included endpoints for stocks, FIIs, cryptocurrencies, historical data, and stock dividends. Web scraping from a FIIs assets site Focused solely on FIIs data. The idea here was to demonstrate that our solution could ingest data in three formats: APIs, web scraping, and legacy databases. I will detail this in the ingestion section. This approach was experimental, and the logic developed could be replicated based on these established bases. Ingestion With the data sources defined, the initial step was to construct three AWS Lambda functions: Stock Data Lambda : Handles data for 10 stocks due to API call limit restrictions. Crypto Data Lambda : Initially manages data for only 2 assets. FIIs Data Lambda (Web Scraping): Scrapes approximately 30 assets per run. Database Setup on AWS RDS Following the Lambda setup, a PostgreSQL database was established on AWS RDS. This setup was intended to simulate the migration from a legacy database system . The database includes historical data and dividend information for stocks. The aim was to address data ingestions from the most common types of sources during the ingestion phase, demonstrating the versatility of the serverless architecture. Process/Transform With the data ingested into AWS from the previous step, AWS Glue was chosen as the processing and transformation resource. This choice was made because Glue supports Spark and allows for the use of Python. Considering that the solution needed to scale effortlessly, implementing it with Spark from the outset significantly increases the update frequency of the Lambdas and allows scaling the number of Lambdas without impacting the performance of the transformation and processing stage. Developed Glue Jobs Five jobs were developed in Glue: Stocks Raw - Primarily involves removing duplicates from the source and saving these files in delta format. Webscrap Raw - Involves removing duplicates from the source and saving these files in delta format. Crypto Raw - Involves removing duplicates from the source and saving these files in delta format. Historical Dividend - Merges (with a join) the output of the first job with historical data that came from the DMS migration. Stocks Refined - Involves column selection, data typing treatment, and creation of calculated fields, simulating a trusted table being made available to the business. \ud83d\udd0e NOTE : If you look at the codes, you will see only 3 scripts, because a way was developed for the first three to use the same script base, changing only their variables. All of this is automatically deployed via Terraform (variables / terraform.tfvars [hidden \ud83d\udd12]). Data Storage Data Storage Strategy Data storage is a critical component at nearly every stage of the project, implemented using different object stores (AWS S3 buckets) according to the stage, following the best practices of Medallion architecture type. Data were segregated into three layers: Landing - Stores the ingested data in JSON format. Bronze - Stores data without duplicates in delta format. Silver - Stores data with joins, calculated columns, and business-specific columns. \ud83d\udd0e NOTE : A Gold layer was not constructed because such layers generally require a project to be at a more advanced stage. Often, new projects are fed by tables from the Silver zone - trusted or refined. Additionally, the project development time was short, and there wasn\u2019t enough time to consolidate a modeling strategy. Having separate buckets for different layers offers several advantages, such as having a more historical layer and a more analytical one. As the project evolves, it becomes possible to determine the necessary range of data for generating value, allowing for different configurations in the S3 buckets, such as having a replica for backup purposes in the historical layer. While not applied here, it is also possible to configure a range (based on dates) to store older data in a colder storage layer, meaning these less accessed data will essentially hibernate, reducing costs. These are some of the advantages of the implemented approach, and as the project progresses and more is understood about the business, even more features aimed at performance and cost reduction can be configured. Delta Format In addition to the above optimizations, the Delta file format was utilized for the analytical layers, enhancing performance and reducing costs through ACID transactions and schema enforcement. These features maintain data integrity and consistency and support efficient querying with mechanisms like file skipping and Z-ordering. While not all advanced features were leveraged (becuase of the time), significant learning were gained regarding the fundamental aspects of Delta Lake and its practical application within the AWS Glue environment (not trivial like Dbricks). This strategic decision supports transitioning towards a lakehouse architecture, which optimizes both batch and real-time data processing, and provides a scalable framework for future enhancements. Data Serving Once all the data has been processed and transformed, the AWS Athena tool can access these tables, which were crawled by the Glue Crawler. This setup recognizes data structures and can significantly benefit from potential performance and cost optimizations. However, it's important to note that in this specific instance, optimizations using Hive or Delta for query enhancements were not utilized due to time constraints, although such enhancements are certainly possible. Following this, an EC2 instance with 8 GB of RAM and 2 CPUs is launched to support at least 10 analysts and dashboards. This setup, which is quite performant, likely exceeds the current needs. On this Linux machine, PostgreSQL and Metabase are installed. PostgreSQL stores the state and volume data for Metabase. With Metabase operational, a quick and intuitive configuration process is carried out, including the creation of user accounts and connections. Metabase is directly linked to Athena, ensuring that every query run accesses data through Athena and is served via Metabase\u2019s interface. This setup allows data or business analysts to access data without needing to directly interact with AWS, and facilitates the publishing of dashboards and queries to Google Sheets and Google Presentations. This architecture not only secures high-performance data querying and low-cost operations on the silver data layers but also enhances accessibility and usability for analysts through a user-friendly dashboard interface. \ud83c\udfc1 NOTE Here we reach the conclusion of the main points that were developed. Below, some descriptions about the remaining items will be provided, but there will be more discussions about the learnings encountered in the results session. Others \ud83d\udcce Orchestration In this project, two other serverless resources were utilized, which together function similarly to Airflow. However, given the convenience of creating DAGs with AWS Step Functions and the limited time factor, this was the chosen approach for now. In the cost section, it will be seen that this is undoubtedly a good option in this analysis, but in terms of management, it is believed that Airflow is more robust and a more complete tool. However, the choice was also about seizing an opportunity to learn, experiment, and particularly value time efficiency. It's worth noting that this tool has an easy implementation and monitoring of DAGs similar to Airflow. Below is the dashboard showing the latest runs of a pipeline orchestrated by Step Functions: Data Quality & Observability AWS already hosts a variety of services that are essential for two crucial aspects of data lifecycle management: data quality and observability. However, the focus was more on utilizing CloudWatch and Datacatalog. Additionally, there was an exploration into developing a custom tool tailored for specific needs. It's understood that there were numerous constraints in previous roles which shaped this approach. Firstly, using the Pydantic library, a sort of data contract was created, specifying some columns as mandatory during data ingestion to ensure data quality with key fields always populated. Secondly, within all the Lambdas and some Glue scripts, a method was devised to monitor pertinent metrics, such as process time, file size, and log collection, verifying if they succeeded according to the set logic. These logs are stored in an S3 environment, accessed via boto3 within Docker, and visualized in Streamlit. While recognizing that this approach isn't the most efficient compared to existing, superior solutions, it was more about highlighting the importance of these metrics and prioritizing my time effectively.","title":"Solution"},{"location":"p2_solution/#solution","text":"","title":"Solution"},{"location":"p2_solution/#aws-serverless-architecture","text":"\ud83d\udcbb Note : If you are not familiar with what serverless resources are, the topic will be discussed in the price section and additional materials will be provided in the reference section. The goal here is not to delve into every detail of what was used, but rather to explain the core elements of the solution and some details about the concepts and tools employed. We will break it down into parts to make it easier to understand each stage.","title":"AWS Serverless Architecture"},{"location":"p2_solution/#data-sources","text":"For our project, we primarily used two sources (with multiple endpoints), as one might expect a financial solution to operate. We utilized a paid API that ensures data curation, availability (without slowdowns during peak hours), and high-frequency updates. \ud83d\udcbb Note : Relying solely on public sources and web scraping does not guarantee quality and also demands tremendous effort. Cost-effective API solutions exist and are quite affordable; for instance, an enterprise-level service might cost about BRL 200 per month. For our volume and frequency, a simple service costing BRL 20 per month sufficed, saving me a great deal of time and the risk of source failure (in cases where only web scraping is used).","title":"Data Sources"},{"location":"p2_solution/#data-sources-used","text":"Brapi API This included endpoints for stocks, FIIs, cryptocurrencies, historical data, and stock dividends. Web scraping from a FIIs assets site Focused solely on FIIs data. The idea here was to demonstrate that our solution could ingest data in three formats: APIs, web scraping, and legacy databases. I will detail this in the ingestion section. This approach was experimental, and the logic developed could be replicated based on these established bases.","title":"Data Sources Used:"},{"location":"p2_solution/#ingestion","text":"With the data sources defined, the initial step was to construct three AWS Lambda functions: Stock Data Lambda : Handles data for 10 stocks due to API call limit restrictions. Crypto Data Lambda : Initially manages data for only 2 assets. FIIs Data Lambda (Web Scraping): Scrapes approximately 30 assets per run. Database Setup on AWS RDS Following the Lambda setup, a PostgreSQL database was established on AWS RDS. This setup was intended to simulate the migration from a legacy database system . The database includes historical data and dividend information for stocks. The aim was to address data ingestions from the most common types of sources during the ingestion phase, demonstrating the versatility of the serverless architecture.","title":"Ingestion"},{"location":"p2_solution/#processtransform","text":"With the data ingested into AWS from the previous step, AWS Glue was chosen as the processing and transformation resource. This choice was made because Glue supports Spark and allows for the use of Python. Considering that the solution needed to scale effortlessly, implementing it with Spark from the outset significantly increases the update frequency of the Lambdas and allows scaling the number of Lambdas without impacting the performance of the transformation and processing stage.","title":"Process/Transform"},{"location":"p2_solution/#developed-glue-jobs","text":"Five jobs were developed in Glue: Stocks Raw - Primarily involves removing duplicates from the source and saving these files in delta format. Webscrap Raw - Involves removing duplicates from the source and saving these files in delta format. Crypto Raw - Involves removing duplicates from the source and saving these files in delta format. Historical Dividend - Merges (with a join) the output of the first job with historical data that came from the DMS migration. Stocks Refined - Involves column selection, data typing treatment, and creation of calculated fields, simulating a trusted table being made available to the business. \ud83d\udd0e NOTE : If you look at the codes, you will see only 3 scripts, because a way was developed for the first three to use the same script base, changing only their variables. All of this is automatically deployed via Terraform (variables / terraform.tfvars [hidden \ud83d\udd12]).","title":"Developed Glue Jobs"},{"location":"p2_solution/#data-storage","text":"","title":"Data Storage"},{"location":"p2_solution/#data-storage-strategy","text":"Data storage is a critical component at nearly every stage of the project, implemented using different object stores (AWS S3 buckets) according to the stage, following the best practices of Medallion architecture type. Data were segregated into three layers: Landing - Stores the ingested data in JSON format. Bronze - Stores data without duplicates in delta format. Silver - Stores data with joins, calculated columns, and business-specific columns. \ud83d\udd0e NOTE : A Gold layer was not constructed because such layers generally require a project to be at a more advanced stage. Often, new projects are fed by tables from the Silver zone - trusted or refined. Additionally, the project development time was short, and there wasn\u2019t enough time to consolidate a modeling strategy. Having separate buckets for different layers offers several advantages, such as having a more historical layer and a more analytical one. As the project evolves, it becomes possible to determine the necessary range of data for generating value, allowing for different configurations in the S3 buckets, such as having a replica for backup purposes in the historical layer. While not applied here, it is also possible to configure a range (based on dates) to store older data in a colder storage layer, meaning these less accessed data will essentially hibernate, reducing costs. These are some of the advantages of the implemented approach, and as the project progresses and more is understood about the business, even more features aimed at performance and cost reduction can be configured.","title":"Data Storage Strategy"},{"location":"p2_solution/#delta-format","text":"In addition to the above optimizations, the Delta file format was utilized for the analytical layers, enhancing performance and reducing costs through ACID transactions and schema enforcement. These features maintain data integrity and consistency and support efficient querying with mechanisms like file skipping and Z-ordering. While not all advanced features were leveraged (becuase of the time), significant learning were gained regarding the fundamental aspects of Delta Lake and its practical application within the AWS Glue environment (not trivial like Dbricks). This strategic decision supports transitioning towards a lakehouse architecture, which optimizes both batch and real-time data processing, and provides a scalable framework for future enhancements.","title":"Delta Format"},{"location":"p2_solution/#data-serving","text":"Once all the data has been processed and transformed, the AWS Athena tool can access these tables, which were crawled by the Glue Crawler. This setup recognizes data structures and can significantly benefit from potential performance and cost optimizations. However, it's important to note that in this specific instance, optimizations using Hive or Delta for query enhancements were not utilized due to time constraints, although such enhancements are certainly possible. Following this, an EC2 instance with 8 GB of RAM and 2 CPUs is launched to support at least 10 analysts and dashboards. This setup, which is quite performant, likely exceeds the current needs. On this Linux machine, PostgreSQL and Metabase are installed. PostgreSQL stores the state and volume data for Metabase. With Metabase operational, a quick and intuitive configuration process is carried out, including the creation of user accounts and connections. Metabase is directly linked to Athena, ensuring that every query run accesses data through Athena and is served via Metabase\u2019s interface. This setup allows data or business analysts to access data without needing to directly interact with AWS, and facilitates the publishing of dashboards and queries to Google Sheets and Google Presentations. This architecture not only secures high-performance data querying and low-cost operations on the silver data layers but also enhances accessibility and usability for analysts through a user-friendly dashboard interface. \ud83c\udfc1 NOTE Here we reach the conclusion of the main points that were developed. Below, some descriptions about the remaining items will be provided, but there will be more discussions about the learnings encountered in the results session.","title":"Data Serving"},{"location":"p2_solution/#others","text":"","title":"Others \ud83d\udcce"},{"location":"p2_solution/#orchestration","text":"In this project, two other serverless resources were utilized, which together function similarly to Airflow. However, given the convenience of creating DAGs with AWS Step Functions and the limited time factor, this was the chosen approach for now. In the cost section, it will be seen that this is undoubtedly a good option in this analysis, but in terms of management, it is believed that Airflow is more robust and a more complete tool. However, the choice was also about seizing an opportunity to learn, experiment, and particularly value time efficiency. It's worth noting that this tool has an easy implementation and monitoring of DAGs similar to Airflow. Below is the dashboard showing the latest runs of a pipeline orchestrated by Step Functions:","title":"Orchestration"},{"location":"p2_solution/#data-quality-observability","text":"AWS already hosts a variety of services that are essential for two crucial aspects of data lifecycle management: data quality and observability. However, the focus was more on utilizing CloudWatch and Datacatalog. Additionally, there was an exploration into developing a custom tool tailored for specific needs. It's understood that there were numerous constraints in previous roles which shaped this approach. Firstly, using the Pydantic library, a sort of data contract was created, specifying some columns as mandatory during data ingestion to ensure data quality with key fields always populated. Secondly, within all the Lambdas and some Glue scripts, a method was devised to monitor pertinent metrics, such as process time, file size, and log collection, verifying if they succeeded according to the set logic. These logs are stored in an S3 environment, accessed via boto3 within Docker, and visualized in Streamlit. While recognizing that this approach isn't the most efficient compared to existing, superior solutions, it was more about highlighting the importance of these metrics and prioritizing my time effectively.","title":"Data Quality &amp; Observability"},{"location":"p3_ing/","text":"Ingestion Framework This section highlights the development aspects of data ingestion and the initial processing layer. A framework was established for data ingestion, designed to facilitate the addition of new sources quickly, emphasizing the importance of standardization and step-by-step guidance for creating and replicating new Lambdas, which proved extremely useful in this context. Lambdas Lambdas, essentially serverless functions (managed by AWS, which takes care of the infrastructure and resources needed), can be configured with basic settings like the programming language and system architecture (x86_64). However, those familiar with Lambdas often encounter lengthy and complex scripts that are challenging to read and debug. The goal was to standardize a simple and organized way to develop and deploy these functions. Initially, standard function modules/classes were created for commonly used functions/instances in this context, such as sending received API JSON to a bucket, API interaction (which might vary depending on the API endpoint), data typing verification with Pydantic (subject to changes), log creation, and automatic sending to a bucket, among other functionalities. Typically, Lambdas are developed locally due to convenience (even using notebooks to speed up development), so these common libraries are automatically imported regardless of the directory path (to keep deployment organized). Lambda functions always follow the same structure to reuse functions and ensure correct log collection. Scripts were designed to eliminate all hard-coded variables, where each used variable would be accessed from a separate configuration file, allowing easy replication of a Lambda with the same endpoint and facilitating code readability and variable checking. With the script functional, the deployment stage follows. Utilizing the earlier steps, a straightforward method to deploy compound Lambdas (comprising more than one file) is through zipping. With this approach, common resources are already inside the standard zip, which is copied to a new directory where lambda_function.py/config.py and possibly another file with specific requirements are updated. With all the points above addressed, the rest involves simply replicating the resource with necessary configurations via Terraform, requiring only the path of the zip file. This ensures high ease of replication for similar services, where, for instance, only the assets in the config file need to be added or modified. This setup is integrated into the Step Functions of Terraform, enabling high speed and potential for automation. The final idea, which was not developed due to time constraints, involved using a form input to check for ingested stocks via an endpoint. If a particular asset was missing, a bash script would create the configuration file and replicate the resource automatically. For cases involving new pipelines or different endpoints, this approach remains highly beneficial as it always allows for reuse of components and leverages Infrastructure as Code (IaC) to save many configurations that would be time-consuming through the interface. Additionally, some practices were adapted for use in Glue, where the three bronze jobs utilize the same script, with only variables in the Terraform files being adjusted. Thus, at least in the initial processing stage, development speed and standardization are also ensured, practices that are highly valuable and consistently add significant value to the project when well implemented.","title":"Ingestion Framework"},{"location":"p3_ing/#ingestion-framework","text":"This section highlights the development aspects of data ingestion and the initial processing layer. A framework was established for data ingestion, designed to facilitate the addition of new sources quickly, emphasizing the importance of standardization and step-by-step guidance for creating and replicating new Lambdas, which proved extremely useful in this context.","title":"Ingestion Framework"},{"location":"p3_ing/#lambdas","text":"Lambdas, essentially serverless functions (managed by AWS, which takes care of the infrastructure and resources needed), can be configured with basic settings like the programming language and system architecture (x86_64). However, those familiar with Lambdas often encounter lengthy and complex scripts that are challenging to read and debug. The goal was to standardize a simple and organized way to develop and deploy these functions. Initially, standard function modules/classes were created for commonly used functions/instances in this context, such as sending received API JSON to a bucket, API interaction (which might vary depending on the API endpoint), data typing verification with Pydantic (subject to changes), log creation, and automatic sending to a bucket, among other functionalities. Typically, Lambdas are developed locally due to convenience (even using notebooks to speed up development), so these common libraries are automatically imported regardless of the directory path (to keep deployment organized). Lambda functions always follow the same structure to reuse functions and ensure correct log collection. Scripts were designed to eliminate all hard-coded variables, where each used variable would be accessed from a separate configuration file, allowing easy replication of a Lambda with the same endpoint and facilitating code readability and variable checking. With the script functional, the deployment stage follows. Utilizing the earlier steps, a straightforward method to deploy compound Lambdas (comprising more than one file) is through zipping. With this approach, common resources are already inside the standard zip, which is copied to a new directory where lambda_function.py/config.py and possibly another file with specific requirements are updated. With all the points above addressed, the rest involves simply replicating the resource with necessary configurations via Terraform, requiring only the path of the zip file. This ensures high ease of replication for similar services, where, for instance, only the assets in the config file need to be added or modified. This setup is integrated into the Step Functions of Terraform, enabling high speed and potential for automation. The final idea, which was not developed due to time constraints, involved using a form input to check for ingested stocks via an endpoint. If a particular asset was missing, a bash script would create the configuration file and replicate the resource automatically. For cases involving new pipelines or different endpoints, this approach remains highly beneficial as it always allows for reuse of components and leverages Infrastructure as Code (IaC) to save many configurations that would be time-consuming through the interface. Additionally, some practices were adapted for use in Glue, where the three bronze jobs utilize the same script, with only variables in the Terraform files being adjusted. Thus, at least in the initial processing stage, development speed and standardization are also ensured, practices that are highly valuable and consistently add significant value to the project when well implemented.","title":"Lambdas"},{"location":"p4_meta/","text":"Metabase / Dashboard Metabase Initially, there was an interest in experimenting with Apache Superset, but due to limited materials on connecting it with Athena, the decision was made to switch to Metabase. Opting out of Power BI was intentional, as the aim was to explore a different tool that could utilize a Docker image, furthering learning experiences on AWS. The choice of Metabase proved rewarding, revisiting it with more knowledge gained since first using it as a data consumer during an internship in data science. Therefore, let's highlight the advantages of this choice: Imagining a scenario where a company does not adhere to the standard Microsoft stack and aims for agility and cost reduction, Metabase emerges as an excellent choice. From a cost perspective, for 10 analyst users plus dashboards, the expense was estimated at only $30 per month, whereas Power BI would cost $100 for the same number of users. The installation process is relatively straightforward, running a docker-compose on EC2 with some additional configurations, and quickly, it's up and running with a PostgreSQL database maintaining its state and managing its volume. Once operational, configuring it via their interface is swift, allowing the creation of different access groups, such as query creators or readers of pre-made queries, etc. There's a wealth of settings available aimed at enforcing security best practices as well as cost and performance optimization. Queries are entirely managed through SQL, which is beneficial from a language standpoint, avoiding the need to learn the more peculiar syntax often associated with data visualization tools. In terms of performance, Metabase uses Athena as its querying engine, essentially building a platform on top of Athena, which ensures high performance. With Athena's metadata, it's possible to track which queries are most frequently run and devise strategies to reduce costs. This integration not only enhances performance but also aligns with efficient data management practices. In Metabase, publishing or embedding questions is straightforward and effortless, enhancing delivery speed and ensuring the reuse of questions across different dashboards. This efficiency significantly reduces development time. Such features are particularly well-suited to the context of an initial data project that needs to be structured and deliver value quickly. This capability allows teams to rapidly iterate on insights and visualize trends without needing to recreate queries for different analytical perspectives. Dashboard Due to security concerns, the link to the Metabase dashboard will not be shared. For access, all ports were left open, prioritizing ease of exploration over security. This was a deliberate choice to experiment with the tool without extensive setup. However, it's important to remember that in a professional setting, several security measures can be implemented, such as embedding the dashboard within a secured website that requires login, or allowing read-only access within Metabase. This approach ensures that the dashboard is accessible via the web, making it easy to share with clients or colleagues as needed, while highlighting the flexibility of Metabase for quick deployment and iterative testing in data-driven projects. Additionally, the analyses conducted were somewhat random, with the primary intention of exploring the tool's capabilities rather than diving deeply into business analytics or generating substantial insights. This approach was taken due to time constraints and the exploratory nature of the project. The main purpose of this dashboard is to complete the project cycle\u2014from data source to data serving\u2014enabling business analysts who understand the domain to engage in meaningful discussions and perform analyses that deliver value to the client. This setup allows for quick setup and iterative exploration, which is crucial in early stages of data projects where the focus is on structuring and rapid value delivery.","title":"Metabase / Dashboard"},{"location":"p4_meta/#metabase-dashboard","text":"","title":"Metabase / Dashboard"},{"location":"p4_meta/#metabase","text":"Initially, there was an interest in experimenting with Apache Superset, but due to limited materials on connecting it with Athena, the decision was made to switch to Metabase. Opting out of Power BI was intentional, as the aim was to explore a different tool that could utilize a Docker image, furthering learning experiences on AWS. The choice of Metabase proved rewarding, revisiting it with more knowledge gained since first using it as a data consumer during an internship in data science. Therefore, let's highlight the advantages of this choice: Imagining a scenario where a company does not adhere to the standard Microsoft stack and aims for agility and cost reduction, Metabase emerges as an excellent choice. From a cost perspective, for 10 analyst users plus dashboards, the expense was estimated at only $30 per month, whereas Power BI would cost $100 for the same number of users. The installation process is relatively straightforward, running a docker-compose on EC2 with some additional configurations, and quickly, it's up and running with a PostgreSQL database maintaining its state and managing its volume. Once operational, configuring it via their interface is swift, allowing the creation of different access groups, such as query creators or readers of pre-made queries, etc. There's a wealth of settings available aimed at enforcing security best practices as well as cost and performance optimization. Queries are entirely managed through SQL, which is beneficial from a language standpoint, avoiding the need to learn the more peculiar syntax often associated with data visualization tools. In terms of performance, Metabase uses Athena as its querying engine, essentially building a platform on top of Athena, which ensures high performance. With Athena's metadata, it's possible to track which queries are most frequently run and devise strategies to reduce costs. This integration not only enhances performance but also aligns with efficient data management practices. In Metabase, publishing or embedding questions is straightforward and effortless, enhancing delivery speed and ensuring the reuse of questions across different dashboards. This efficiency significantly reduces development time. Such features are particularly well-suited to the context of an initial data project that needs to be structured and deliver value quickly. This capability allows teams to rapidly iterate on insights and visualize trends without needing to recreate queries for different analytical perspectives.","title":"Metabase"},{"location":"p4_meta/#dashboard","text":"Due to security concerns, the link to the Metabase dashboard will not be shared. For access, all ports were left open, prioritizing ease of exploration over security. This was a deliberate choice to experiment with the tool without extensive setup. However, it's important to remember that in a professional setting, several security measures can be implemented, such as embedding the dashboard within a secured website that requires login, or allowing read-only access within Metabase. This approach ensures that the dashboard is accessible via the web, making it easy to share with clients or colleagues as needed, while highlighting the flexibility of Metabase for quick deployment and iterative testing in data-driven projects. Additionally, the analyses conducted were somewhat random, with the primary intention of exploring the tool's capabilities rather than diving deeply into business analytics or generating substantial insights. This approach was taken due to time constraints and the exploratory nature of the project. The main purpose of this dashboard is to complete the project cycle\u2014from data source to data serving\u2014enabling business analysts who understand the domain to engage in meaningful discussions and perform analyses that deliver value to the client. This setup allows for quick setup and iterative exploration, which is crucial in early stages of data projects where the focus is on structuring and rapid value delivery.","title":"Dashboard"},{"location":"p5_price/","text":"Price Development cost \ud83d\udccb Based on the AWS billing estimate, the development environment is projected to cost $38 by the end of June. This budget accounts for a development phase filled with trial and error and various experiments without exceeding $50. \ud83d\udcbb Note : A $300 AWS experimentation coupon was received, which facilitated a more fearless approach to experimenting in this proof of concept (PoC). This financial buffer allowed for extensive testing and exploration without significant cost concerns. While the current development setup is estimated to cost $38 by the end of June, this scenario primarily serves to demonstrate the functionality of the solution in a controlled, low-cost environment. However, there's a recognition that the costs for deploying the solution in a production environment, particularly for a micro or small business, might be significantly different. To address this, there's been a proposal to project future costs when scaling the solution to a production setting. Production Price Projection \ud83d\udcb5 The concept here was indeed to increase the volume of incoming data and the frequency of runs in the pipeline for the proof of concept development. Initially, there were effectively four data sources, one of which was solely to demonstrate data migration from legacy databases\u2014highlighting the versatility of the solution. Thus, essentially three Lambdas were operational: one for stocks and FIIs (via the BRAPI API), a second for web scraping real estate funds, and a third for cryptocurrencies (also via the BRAPI API). Within these Lambdas, approximately thirty assets were handled by the web scraping function, five by the cryptocurrency function, and ten by the stocks function. For a scenario projecting around 1000 assets, it would require scaling up to about one hundred Lambdas (with an allocation of 70% for stocks/FIIs, 20% for web scraping, and 10% for cryptocurrencies) to maintain efficiency. Considering the update frequency\u2014given that the target personas are primarily holders, who don't require frequent updates\u2014a rate of ten updates per day was deemed satisfactory. It's feasible to run numerous distinct Lambdas with a single instance of AWS Glue processing similar types of data, such as different stocks pulled from the same API. With these assumptions in mind, the cost projection involves configuring the resources appropriately and calculating the total number of executions per month. This would give a clearer picture of the operational costs expected when scaling up the solution for a micro or small business in a production environment. This strategic planning is crucial to balance performance needs with cost efficiency. price calculated on June 19, in the US-East-2 region There are some additional resources not listed here, but the total for the rest (Step Functions, Amazon Bridge Scheduler, VPS, etc.) does not even reach $5 per month. Lambda Calculations were based on some Lambdas running more than ten times a day. Even with each Lambda running over 100 times daily, we will still remain within the free tier: 100 (Lambdas) * 100 (runs) * 30 (days) = 300,000 requests. Hence, Lambda usage for our case is essentially free. S3 (Object Store) This is our storage layer. Calculations were made assuming data volumes of 1TB, with partial replicas for more critical data (backup), with many requests (300K per month). Given that our current volume is much lower due to the business nature, the cost is expected to be significantly lower, especially in the first year of production. Since our data will be consumed by Athena, this greatly reduces expenses on external transfers, which are usually the costliest aspect of any Object Store. Athena Queries were estimated for sweeping through hundreds of thousands of lines (about 100 MB), and I estimated how many queries an analyst might make per day, arriving at about 32. Even multiplied by 10 analysts, we are well below the 2,000 queries I projected, thinking about having dashboards in production, business queries, data science, etc. There is still much room to optimize and further reduce this cost, but even with higher estimates and minimal optimization, the cost is quite low. EC2 Our Metabase is hosted on EC2, where I tried to project a machine that can support at least 10 analysts querying simultaneously and sustain the dashboards in production. As previously mentioned, these $30 per month are much cheaper than at least $100 (just for the analysts) with PowerBI. Glue The calculator shows the price of each Run with our current settings. I estimated the execution time high; currently, runs take about 50 to 62 seconds max, but I set it to 180 seconds to account for an increase in volume. Remember, we have several potential optimizations to make in the future to reduce this value significantly. However, as this was a v0 simulation, I wanted to estimate this price with a higher data volume. We currently have 5 Glues running daily, which could easily support all 100 Lambdas with minor adaptations (literally zero modifications for stocks). Thus: 5 (Lambdas) * 10 (runs/day) * 30 days = 1,500 runs * $0.11 This comes to approximately $165 per month. Indeed, the processing stage is the most expensive we have, but it remains cost-effective, given that we are dealing with highly scalable distributed computing without worrying about almost any cluster and system configuration settings. Total Price $25 (S3) + $29 (Athena) + $30 (EC2) + $6 (other services) + $165 (Glue) $255 per month | $3,060 per year Glue/Serverless price discussion \ud83d\udd0d Consultations with several senior professionals in the field have validated that the overall pricing of our architecture is exceptionally competitive, even for solutions aimed at micro and small businesses. It's important to note that all resource costs have been estimated high, and there remains significant room for optimization. A particular point of interest is that making these high estimates without extensive testing on Glue has likely inflated the price, as various optimizations could be implemented during the move to production to reduce costs. Furthermore, the lack of data on runs with higher volumes means that the projection for Spark's performance and time requirements is more qualitative. It\u2019s also worth discussing whether, given these costs, it might be advisable to have a dedicated resource for our Spark jobs. The answer isn't straightforward. When discussing serverless architecture, some minor configurations are necessary, but ultimately, AWS handles the cluster configuration and ensures their availability, which justifies the higher service cost compared to managing your own resources. However, in startup scenarios and certain contexts, the cost for EMR services can initially be more expensive than the serverless solution. A review of some EMR configurations revealed that prices start at around $140 per month for a single core setup. This discussion highlights the trade-offs between cost, management overhead, and scalability when choosing between serverless architectures and dedicated resources like EMR. Further exploration and cost-benefit analysis might be required to determine the best approach for specific business needs and growth stages. This section delves into the finer details of the solution: Load Tolerance: Is the cluster really going to handle the maximum load during peak hours? Upgrade Necessity: Might there be a need to upgrade or evolve this cluster to handle increased loads (more costly configuration)? Monitoring and Configuration Time: How much time will be spent monitoring and actually finding a configuration that matches the company's usage? Engineering Time: How many hours will engineers spend configuring this? EMR Usage Time: Will using EMR require more hours for each pipeline? The discussion isn't merely about raw pricing. It also involves the cost of specialized human resources. The intent here is not just to advocate that serverless is better but to broaden the discussion beyond simple cost calculations because every business has its own pace and needs. The point is raised because, in the future, it might be valid to move away from Glue to keep a better track of expenses. More customized tools tend to be less expensive, and there are several current solutions that could be implemented with a focus on costs. For example, using Terraform/GitOps to spin up an EMR at scheduled times to run jobs and then tear everything down, thus only paying for processing hours, could significantly reduce costs. However, this approach introduces greater complexity and requires a well-established business model since such a solution demands more time and labor to develop (more investment in human work hours). To conclude this section, it's fitting to reflect on a powerful statement by the famous YouTuber, Fireship: \"In the cloud, we don't have solutions, only trade-offs.\" This emphasizes that there isn't a one-size-fits-all solution in cloud computing. Instead, it's a continual balancing act, where we must remain vigilant and make decisions that best fit the business needs, understanding where we are willing to compromise ($$$) more.","title":"Price"},{"location":"p5_price/#price","text":"","title":"Price"},{"location":"p5_price/#development-cost","text":"Based on the AWS billing estimate, the development environment is projected to cost $38 by the end of June. This budget accounts for a development phase filled with trial and error and various experiments without exceeding $50. \ud83d\udcbb Note : A $300 AWS experimentation coupon was received, which facilitated a more fearless approach to experimenting in this proof of concept (PoC). This financial buffer allowed for extensive testing and exploration without significant cost concerns. While the current development setup is estimated to cost $38 by the end of June, this scenario primarily serves to demonstrate the functionality of the solution in a controlled, low-cost environment. However, there's a recognition that the costs for deploying the solution in a production environment, particularly for a micro or small business, might be significantly different. To address this, there's been a proposal to project future costs when scaling the solution to a production setting.","title":"Development cost \ud83d\udccb"},{"location":"p5_price/#production-price-projection","text":"The concept here was indeed to increase the volume of incoming data and the frequency of runs in the pipeline for the proof of concept development. Initially, there were effectively four data sources, one of which was solely to demonstrate data migration from legacy databases\u2014highlighting the versatility of the solution. Thus, essentially three Lambdas were operational: one for stocks and FIIs (via the BRAPI API), a second for web scraping real estate funds, and a third for cryptocurrencies (also via the BRAPI API). Within these Lambdas, approximately thirty assets were handled by the web scraping function, five by the cryptocurrency function, and ten by the stocks function. For a scenario projecting around 1000 assets, it would require scaling up to about one hundred Lambdas (with an allocation of 70% for stocks/FIIs, 20% for web scraping, and 10% for cryptocurrencies) to maintain efficiency. Considering the update frequency\u2014given that the target personas are primarily holders, who don't require frequent updates\u2014a rate of ten updates per day was deemed satisfactory. It's feasible to run numerous distinct Lambdas with a single instance of AWS Glue processing similar types of data, such as different stocks pulled from the same API. With these assumptions in mind, the cost projection involves configuring the resources appropriately and calculating the total number of executions per month. This would give a clearer picture of the operational costs expected when scaling up the solution for a micro or small business in a production environment. This strategic planning is crucial to balance performance needs with cost efficiency. price calculated on June 19, in the US-East-2 region There are some additional resources not listed here, but the total for the rest (Step Functions, Amazon Bridge Scheduler, VPS, etc.) does not even reach $5 per month.","title":"Production Price Projection \ud83d\udcb5"},{"location":"p5_price/#lambda","text":"Calculations were based on some Lambdas running more than ten times a day. Even with each Lambda running over 100 times daily, we will still remain within the free tier: 100 (Lambdas) * 100 (runs) * 30 (days) = 300,000 requests. Hence, Lambda usage for our case is essentially free.","title":"Lambda"},{"location":"p5_price/#s3-object-store","text":"This is our storage layer. Calculations were made assuming data volumes of 1TB, with partial replicas for more critical data (backup), with many requests (300K per month). Given that our current volume is much lower due to the business nature, the cost is expected to be significantly lower, especially in the first year of production. Since our data will be consumed by Athena, this greatly reduces expenses on external transfers, which are usually the costliest aspect of any Object Store.","title":"S3 (Object Store)"},{"location":"p5_price/#athena","text":"Queries were estimated for sweeping through hundreds of thousands of lines (about 100 MB), and I estimated how many queries an analyst might make per day, arriving at about 32. Even multiplied by 10 analysts, we are well below the 2,000 queries I projected, thinking about having dashboards in production, business queries, data science, etc. There is still much room to optimize and further reduce this cost, but even with higher estimates and minimal optimization, the cost is quite low.","title":"Athena"},{"location":"p5_price/#ec2","text":"Our Metabase is hosted on EC2, where I tried to project a machine that can support at least 10 analysts querying simultaneously and sustain the dashboards in production. As previously mentioned, these $30 per month are much cheaper than at least $100 (just for the analysts) with PowerBI.","title":"EC2"},{"location":"p5_price/#glue","text":"The calculator shows the price of each Run with our current settings. I estimated the execution time high; currently, runs take about 50 to 62 seconds max, but I set it to 180 seconds to account for an increase in volume. Remember, we have several potential optimizations to make in the future to reduce this value significantly. However, as this was a v0 simulation, I wanted to estimate this price with a higher data volume. We currently have 5 Glues running daily, which could easily support all 100 Lambdas with minor adaptations (literally zero modifications for stocks). Thus: 5 (Lambdas) * 10 (runs/day) * 30 days = 1,500 runs * $0.11 This comes to approximately $165 per month. Indeed, the processing stage is the most expensive we have, but it remains cost-effective, given that we are dealing with highly scalable distributed computing without worrying about almost any cluster and system configuration settings.","title":"Glue"},{"location":"p5_price/#total-price","text":"$25 (S3) + $29 (Athena) + $30 (EC2) + $6 (other services) + $165 (Glue) $255 per month | $3,060 per year","title":"Total Price"},{"location":"p5_price/#glueserverless-price-discussion","text":"Consultations with several senior professionals in the field have validated that the overall pricing of our architecture is exceptionally competitive, even for solutions aimed at micro and small businesses. It's important to note that all resource costs have been estimated high, and there remains significant room for optimization. A particular point of interest is that making these high estimates without extensive testing on Glue has likely inflated the price, as various optimizations could be implemented during the move to production to reduce costs. Furthermore, the lack of data on runs with higher volumes means that the projection for Spark's performance and time requirements is more qualitative. It\u2019s also worth discussing whether, given these costs, it might be advisable to have a dedicated resource for our Spark jobs. The answer isn't straightforward. When discussing serverless architecture, some minor configurations are necessary, but ultimately, AWS handles the cluster configuration and ensures their availability, which justifies the higher service cost compared to managing your own resources. However, in startup scenarios and certain contexts, the cost for EMR services can initially be more expensive than the serverless solution. A review of some EMR configurations revealed that prices start at around $140 per month for a single core setup. This discussion highlights the trade-offs between cost, management overhead, and scalability when choosing between serverless architectures and dedicated resources like EMR. Further exploration and cost-benefit analysis might be required to determine the best approach for specific business needs and growth stages. This section delves into the finer details of the solution: Load Tolerance: Is the cluster really going to handle the maximum load during peak hours? Upgrade Necessity: Might there be a need to upgrade or evolve this cluster to handle increased loads (more costly configuration)? Monitoring and Configuration Time: How much time will be spent monitoring and actually finding a configuration that matches the company's usage? Engineering Time: How many hours will engineers spend configuring this? EMR Usage Time: Will using EMR require more hours for each pipeline? The discussion isn't merely about raw pricing. It also involves the cost of specialized human resources. The intent here is not just to advocate that serverless is better but to broaden the discussion beyond simple cost calculations because every business has its own pace and needs. The point is raised because, in the future, it might be valid to move away from Glue to keep a better track of expenses. More customized tools tend to be less expensive, and there are several current solutions that could be implemented with a focus on costs. For example, using Terraform/GitOps to spin up an EMR at scheduled times to run jobs and then tear everything down, thus only paying for processing hours, could significantly reduce costs. However, this approach introduces greater complexity and requires a well-established business model since such a solution demands more time and labor to develop (more investment in human work hours). To conclude this section, it's fitting to reflect on a powerful statement by the famous YouTuber, Fireship: \"In the cloud, we don't have solutions, only trade-offs.\" This emphasizes that there isn't a one-size-fits-all solution in cloud computing. Instead, it's a continual balancing act, where we must remain vigilant and make decisions that best fit the business needs, understanding where we are willing to compromise ($$$) more.","title":"Glue/Serverless price discussion \ud83d\udd0d"},{"location":"p6_results/","text":"Results Scalable Solution The solution is scalable, capable of ingesting new data sources without impacting performance\u2014a key premise from the start of the project. In the financial world, there's a continuous need for new data to add new assets for analysis and client portfolios. The solution ensures this through a developed ingestion framework, providing a simple and agile method to add new assets and endpoints. Flexible Architecture The architecture is malleable, allowing for the scaling up or dismantling of resources without impacting existing setups, fitting new pipelines, or adapting Lambdas for existing pipelines. It has been demonstrated during the project that our architecture can smoothly ingest data in various formats\u2014APIs, web scraping, and databases\u2014through AWS's serverless resources. Additionally, adjusting the update frequency is easily managed in the orchestration stage by merely changing the cron configuration in AWS Step Functions. Data Persistence Across Layers Data is stored across various layers, offering several advantages such as maintaining a real historical data record for legal reasons and more analytical layers focused on performance and cost-efficiency. Understanding business demands, measures can be taken to transfer less frequently used data to even cheaper storage layers (like AWS Glacier), thereby ensuring performance and cost efficiency in the same scenario. Centralized Data Availability via Metabase Data is centralized and made accessible through Metabase, which interfaces with Athena, a high-performance tool capable of accessing Delta-format data, thus enhancing performance and reducing costs. Within Metabase, dashboards can be published and accessed on the web (both PC and mobile) by clients. Data and Business Analysts can also access the refined tables to conduct studies and create dashboards and reports. Data Quality and Observability Observability and understanding of data ingestion are emphasized in the pipelines to enable preventive actions against failures and facilitate effective communication with other departments. FinOps and Cost-Efficient Architecture The architecture is highly cost-effective, capable of handling production scenarios with minimal adjustments. POC Development Cost : Less than $50 Projected Production Cost : $255 per month | $3060 per year \ud83d\udcb8 Potential Business Impact \ud83d\udcb0 Analysts' Time Efficiency It's well known that analysts spend considerable hours collecting and preparing data before they can even begin their actual analysis. After a brief web search, a conservative estimate suggests that each analyst spends around 10 hours per week on these tasks (studies indicate it could be more than 30 hours). For a company with 10 analysts, this equates to about 100 hours per week, or 400 hours per month. Given the average cost of R$40 per hour for a mid-level analyst, the company spends approximately R$16,000 monthly on these preparatory tasks. Cost Savings with the New Solution Our solution, projected to cost about R$1,400 per month, can significantly reduce these expenses, saving the business around R$14,000 per month or R$168,000 annually. This allows funds to be reallocated to marketing or other impactful business activities. Enhanced Analytical Output Moreover, the time saved for analysts can lead to more analytical outcomes, potentially creating additional value for clients and, consequently, the business. This increase in productivity and effectiveness could translate into a competitive advantage in the marketplace.","title":"Results"},{"location":"p6_results/#results","text":"","title":"Results"},{"location":"p6_results/#scalable-solution","text":"The solution is scalable, capable of ingesting new data sources without impacting performance\u2014a key premise from the start of the project. In the financial world, there's a continuous need for new data to add new assets for analysis and client portfolios. The solution ensures this through a developed ingestion framework, providing a simple and agile method to add new assets and endpoints.","title":"Scalable Solution"},{"location":"p6_results/#flexible-architecture","text":"The architecture is malleable, allowing for the scaling up or dismantling of resources without impacting existing setups, fitting new pipelines, or adapting Lambdas for existing pipelines. It has been demonstrated during the project that our architecture can smoothly ingest data in various formats\u2014APIs, web scraping, and databases\u2014through AWS's serverless resources. Additionally, adjusting the update frequency is easily managed in the orchestration stage by merely changing the cron configuration in AWS Step Functions.","title":"Flexible Architecture"},{"location":"p6_results/#data-persistence-across-layers","text":"Data is stored across various layers, offering several advantages such as maintaining a real historical data record for legal reasons and more analytical layers focused on performance and cost-efficiency. Understanding business demands, measures can be taken to transfer less frequently used data to even cheaper storage layers (like AWS Glacier), thereby ensuring performance and cost efficiency in the same scenario.","title":"Data Persistence Across Layers"},{"location":"p6_results/#centralized-data-availability-via-metabase","text":"Data is centralized and made accessible through Metabase, which interfaces with Athena, a high-performance tool capable of accessing Delta-format data, thus enhancing performance and reducing costs. Within Metabase, dashboards can be published and accessed on the web (both PC and mobile) by clients. Data and Business Analysts can also access the refined tables to conduct studies and create dashboards and reports.","title":"Centralized Data Availability via Metabase"},{"location":"p6_results/#data-quality-and-observability","text":"Observability and understanding of data ingestion are emphasized in the pipelines to enable preventive actions against failures and facilitate effective communication with other departments.","title":"Data Quality and Observability"},{"location":"p6_results/#finops-and-cost-efficient-architecture","text":"The architecture is highly cost-effective, capable of handling production scenarios with minimal adjustments. POC Development Cost : Less than $50 Projected Production Cost : $255 per month | $3060 per year \ud83d\udcb8","title":"FinOps and Cost-Efficient Architecture"},{"location":"p6_results/#potential-business-impact","text":"","title":"Potential Business Impact \ud83d\udcb0"},{"location":"p6_results/#analysts-time-efficiency","text":"It's well known that analysts spend considerable hours collecting and preparing data before they can even begin their actual analysis. After a brief web search, a conservative estimate suggests that each analyst spends around 10 hours per week on these tasks (studies indicate it could be more than 30 hours). For a company with 10 analysts, this equates to about 100 hours per week, or 400 hours per month. Given the average cost of R$40 per hour for a mid-level analyst, the company spends approximately R$16,000 monthly on these preparatory tasks.","title":"Analysts' Time Efficiency"},{"location":"p6_results/#cost-savings-with-the-new-solution","text":"Our solution, projected to cost about R$1,400 per month, can significantly reduce these expenses, saving the business around R$14,000 per month or R$168,000 annually. This allows funds to be reallocated to marketing or other impactful business activities.","title":"Cost Savings with the New Solution"},{"location":"p6_results/#enhanced-analytical-output","text":"Moreover, the time saved for analysts can lead to more analytical outcomes, potentially creating additional value for clients and, consequently, the business. This increase in productivity and effectiveness could translate into a competitive advantage in the marketplace.","title":"Enhanced Analytical Output"},{"location":"p7_referen/","text":"References Serverless What is Serverless Computing? AWS https://www.youtube.com/@SoumilShah - For in-depth technical insights on AWS Glue and other related AWS and Lakehouse topics. His content has been a primary technical source for this project. Highly recommended. https://docs.aws.amazon.com/s3/?nc2=h_ql_doc_s3 https://medium.com/cloud-native-daily/building-a-scalable-multi-datasource-etl-platform-with-aws-glue-22f1549995f4 rds_terraform Lakehouse https://juhache.substack.com/p/multi-engine-data-stack-v1?r=l9wvi&triedRedirect=true https://blog.det.life/i-spent-5-hours-understanding-more-about-the-delta-lake-table-format-b8516c5091eb official Delta+glue https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-iceberg.html Metabase https://www.metabase.com/docs/latest/installation-and-operation/running-metabase-on-docker Humor Fireship talks about Serverless","title":"References"},{"location":"p7_referen/#references","text":"","title":"References"},{"location":"p7_referen/#serverless","text":"What is Serverless Computing?","title":"Serverless"},{"location":"p7_referen/#aws","text":"https://www.youtube.com/@SoumilShah - For in-depth technical insights on AWS Glue and other related AWS and Lakehouse topics. His content has been a primary technical source for this project. Highly recommended. https://docs.aws.amazon.com/s3/?nc2=h_ql_doc_s3 https://medium.com/cloud-native-daily/building-a-scalable-multi-datasource-etl-platform-with-aws-glue-22f1549995f4 rds_terraform","title":"AWS"},{"location":"p7_referen/#lakehouse","text":"https://juhache.substack.com/p/multi-engine-data-stack-v1?r=l9wvi&triedRedirect=true https://blog.det.life/i-spent-5-hours-understanding-more-about-the-delta-lake-table-format-b8516c5091eb official Delta+glue https://docs.aws.amazon.com/glue/latest/dg/aws-glue-programming-etl-format-iceberg.html","title":"Lakehouse"},{"location":"p7_referen/#metabase","text":"https://www.metabase.com/docs/latest/installation-and-operation/running-metabase-on-docker","title":"Metabase"},{"location":"p7_referen/#humor","text":"Fireship talks about Serverless","title":"Humor"}]}