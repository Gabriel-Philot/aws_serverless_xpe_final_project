{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Imports 3third party\n",
    "\n",
    "import time\n",
    "import requests\n",
    "import pandas as pd\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "import boto3\n",
    "from datetime import datetime, timedelta\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# IMPORTS/variables custom \n",
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.abspath('.'))\n",
    "\n",
    "load_dotenv()\n",
    "PATH_DIR = os.getenv(\"path_variable\")\n",
    "PROFILE_NAME = os.getenv(\"profile_name\")\n",
    "\n",
    "# Get the absolute path\n",
    "absolute_directory = os.path.abspath(PATH_DIR)\n",
    "\n",
    "# Add the directory to sys.path\n",
    "sys.path.append(absolute_directory)\n",
    "\n",
    "\n",
    "# Imports Resources\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from io import StringIO\n",
    "from datetime import datetime, timedelta\n",
    "from api_requests import Brapi_STOCKS_API, Brapi_notSTOCKS__API\n",
    "#from parquet_model import write_parquet_to_s3_raw\n",
    "from log_manager import S3LogManager\n",
    "#from week_days import get_previous_business_days\n",
    "from util import control_sucess, infer_schema, convert_to_string\n",
    "from json_upload_local import write_json_to_s3\n",
    "from stocks_fiis_modules import validate_data_stocks\n",
    "import time\n",
    "\n",
    "# Import configs_dev in notebook_dir \n",
    "from config_dev import BASE_URL, REGION_NAME, \\\n",
    "    BUCKET, RAW_PREFIX, LOG_PREFIX, NAME_FUN, TOKEN_GENERATOR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# print(\"CHANGE ALL THE MOBILE VARIABLES IN config_dev.py FILE\")\n",
    "# print(f\"{BUCKET} \\n{RAW_PREFIX}\\n {BASE_URL}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# _DEV_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "##  Dev/_Debugging old version_\n",
    "\n",
    "# Develop your code here\n",
    "\n",
    "\n",
    "tickers = [\"PETR4\", \"SUZB3\", \"MGLU3\"]\n",
    "\n",
    "\n",
    "\n",
    "params = {\n",
    "    'range': '1d',\n",
    "    'interval': '1d',\n",
    "    'fundamental': 'true',\n",
    "    'token': TOKEN_GENERATOR,\n",
    "}\n",
    "\n",
    "\n",
    "api = Brapi_STOCKS_API(\n",
    "    url=BASE_URL,\n",
    "    tickers=tickers, \n",
    "    params=params\n",
    ")\n",
    "\n",
    "data = api.get_quotes()\n",
    "schema = infer_schema(data)\n",
    "check_json = validate_data_stocks(data)\n",
    "timestamp_SP = datetime.now() - timedelta(hours=3)\n",
    "#write_json_to_s3(check_json, BUCKET, RAW_PREFIX, timestamp_SP, PROFILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# import pprint\n",
    "# pprint.pprint(check_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda fun model below : Uncode it to dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "\n",
    "# V0 inicial structure for logging ect\n",
    "def lambda_handler(event, context):\n",
    "    inicio = time.perf_counter()\n",
    "    control = \"OK\"\n",
    "    timestamp_SP = datetime.now() - timedelta(hours=3)\n",
    "    timestamp_SP_api_pull = timestamp_SP - timedelta(days=1)\n",
    "    \n",
    "    # loop df ... [!WARNING] if dont use it change in row 58\n",
    "    #df_append = pd.DataFrame()\n",
    "\n",
    "    # api class for requests\n",
    "    tickers = [\"PETR4\", \"SUZB3\", \"MGLU3\"]\n",
    "\n",
    "    params = {\n",
    "        'range': '1d',\n",
    "        'interval': '1d',\n",
    "        'fundamental': 'true',\n",
    "        'token': TOKEN_GENERATOR,\n",
    "    }\n",
    "    \n",
    "    \n",
    "    api = Brapi_STOCKS_API(\n",
    "        url=BASE_URL,\n",
    "        tickers=tickers, \n",
    "        params=params\n",
    "    )\n",
    "    \n",
    "    json_data = api.get_quotes()\n",
    "    \n",
    "    # --------------------Api requests wra wrangling\n",
    "    # ------------------- Use Try and execept\n",
    " \n",
    "    #----------------- execept + control_var for logs management\n",
    "    try:\n",
    "        schema =infer_schema(data)\n",
    "        check_json = validate_data_stocks(json_data)\n",
    "\n",
    "    #----------------- execept + control_var for logs management\n",
    "\n",
    "    \n",
    "    except Exception as e:\n",
    "         control = f'Error msg: {e}'\n",
    "         print(\n",
    "             \"Project: The request returned 0 items. Original message: {msg}\".format(\n",
    "                 msg=e\n",
    "             )\n",
    "         )\n",
    "                        \n",
    "    # -------------------------- usual final strucuture for uploading and logging the ingestion.\n",
    "    \n",
    "    sucess_var = control_sucess(control)\n",
    "\n",
    "    file_s3= 0\n",
    "    if control == \"OK\":\n",
    "        file_s3, file_size = write_json_to_s3(check_json, BUCKET, RAW_PREFIX, timestamp_SP, PROFILE_NAME)\n",
    "        print(f\"Project: Json file saved to S3 at: \\n {file_s3}\")\n",
    "        print(f\"Project: Json file size: {file_size} bytes\")\n",
    "        \n",
    "    fim = time.perf_counter()\n",
    "    log_manager = S3LogManager(BUCKET, LOG_PREFIX, control)\n",
    "    time_var = round((fim - inicio),2)\n",
    "    log_manager.add_log_entry(NAME_FUN, sucess_var, time_var, control, file_size)\n",
    "\n",
    "# ! lambda_handler('','') !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TESTING / DEBUGING IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project: Json file saved to S3 at: \n",
      " s3://aws-serveless-project-lambda-teste/dev/raw/brapi_api/date_ingestion=20240510/file.json\n",
      "Project: Json file size: 2451 bytes\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import logging\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "try:\n",
    "    lambda_handler(None, None)\n",
    "    logging.basicConfig(filename = \"scrapper.log\", level=logging.INFO, format='%(asctime)s - %(message)s', datefmt = '%Y-%m-%d %H:%M:%S')\n",
    "    end = time.perf_counter()\n",
    "    time = end - start_time\n",
    "    logging.info(f\"{NAME_FUN} - success - time {time:.2f}.\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error.{NAME_FUN}:{e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_lighthouse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
